//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-33961263
// Cuda compilation tools, release 12.4, V12.4.99
// Based on NVVM 7.0.1
//

.version 8.4
.target sm_80
.address_size 64

	// .globl	_Z11vecAddNaivePfS_S_i
.extern .shared .align 16 .b8 shared[];

.visible .entry _Z11vecAddNaivePfS_S_i(
	.param .u64 _Z11vecAddNaivePfS_S_i_param_0,
	.param .u64 _Z11vecAddNaivePfS_S_i_param_1,
	.param .u64 _Z11vecAddNaivePfS_S_i_param_2,
	.param .u32 _Z11vecAddNaivePfS_S_i_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd4, [_Z11vecAddNaivePfS_S_i_param_0];
	ld.param.u64 	%rd5, [_Z11vecAddNaivePfS_S_i_param_1];
	ld.param.u64 	%rd6, [_Z11vecAddNaivePfS_S_i_param_2];
	ld.param.u32 	%r6, [_Z11vecAddNaivePfS_S_i_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r7, %r1, %r8;
	setp.ge.u32 	%p1, %r10, %r6;
	@%p1 bra 	$L__BB0_3;

	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd3, %rd6;

$L__BB0_2:
	mul.wide.u32 	%rd7, %r10, 4;
	add.s64 	%rd8, %rd1, %rd7;
	add.s64 	%rd9, %rd2, %rd7;
	ld.global.f32 	%f1, [%rd9];
	ld.global.f32 	%f2, [%rd8];
	add.f32 	%f3, %f2, %f1;
	add.s64 	%rd10, %rd3, %rd7;
	st.global.f32 	[%rd10], %f3;
	add.s32 	%r10, %r10, %r3;
	setp.lt.u32 	%p2, %r10, %r6;
	@%p2 bra 	$L__BB0_2;

$L__BB0_3:
	ret;

}
	// .globl	_Z17vecAddUnrolledBy4PfS_S_i
.visible .entry _Z17vecAddUnrolledBy4PfS_S_i(
	.param .u64 _Z17vecAddUnrolledBy4PfS_S_i_param_0,
	.param .u64 _Z17vecAddUnrolledBy4PfS_S_i_param_1,
	.param .u64 _Z17vecAddUnrolledBy4PfS_S_i_param_2,
	.param .u32 _Z17vecAddUnrolledBy4PfS_S_i_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd4, [_Z17vecAddUnrolledBy4PfS_S_i_param_0];
	ld.param.u64 	%rd5, [_Z17vecAddUnrolledBy4PfS_S_i_param_1];
	ld.param.u64 	%rd6, [_Z17vecAddUnrolledBy4PfS_S_i_param_2];
	ld.param.u32 	%r6, [_Z17vecAddUnrolledBy4PfS_S_i_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r14, %r7, %r1, %r8;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB1_3;

	cvta.to.global.u64 	%rd1, %rd6;
	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd3, %rd4;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r10, %r1, %r9;
	shl.b32 	%r3, %r10, 2;

$L__BB1_2:
	mul.wide.u32 	%rd7, %r14, 4;
	add.s64 	%rd8, %rd3, %rd7;
	add.s64 	%rd9, %rd2, %rd7;
	ld.global.f32 	%f1, [%rd9];
	ld.global.f32 	%f2, [%rd8];
	add.f32 	%f3, %f2, %f1;
	add.s64 	%rd10, %rd1, %rd7;
	st.global.f32 	[%rd10], %f3;
	add.s32 	%r11, %r14, 1;
	mul.wide.u32 	%rd11, %r11, 4;
	add.s64 	%rd12, %rd3, %rd11;
	add.s64 	%rd13, %rd2, %rd11;
	ld.global.f32 	%f4, [%rd13];
	ld.global.f32 	%f5, [%rd12];
	add.f32 	%f6, %f5, %f4;
	add.s64 	%rd14, %rd1, %rd11;
	st.global.f32 	[%rd14], %f6;
	add.s32 	%r12, %r14, 2;
	mul.wide.u32 	%rd15, %r12, 4;
	add.s64 	%rd16, %rd3, %rd15;
	add.s64 	%rd17, %rd2, %rd15;
	ld.global.f32 	%f7, [%rd17];
	ld.global.f32 	%f8, [%rd16];
	add.f32 	%f9, %f8, %f7;
	add.s64 	%rd18, %rd1, %rd15;
	st.global.f32 	[%rd18], %f9;
	add.s32 	%r13, %r14, 3;
	mul.wide.u32 	%rd19, %r13, 4;
	add.s64 	%rd20, %rd3, %rd19;
	add.s64 	%rd21, %rd2, %rd19;
	ld.global.f32 	%f10, [%rd21];
	ld.global.f32 	%f11, [%rd20];
	add.f32 	%f12, %f11, %f10;
	add.s64 	%rd22, %rd1, %rd19;
	st.global.f32 	[%rd22], %f12;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p2, %r14, %r6;
	@%p2 bra 	$L__BB1_2;

$L__BB1_3:
	ret;

}
	// .globl	_Z32vecAddUnrolledBy4ILPMaximizationPfS_S_i
.visible .entry _Z32vecAddUnrolledBy4ILPMaximizationPfS_S_i(
	.param .u64 _Z32vecAddUnrolledBy4ILPMaximizationPfS_S_i_param_0,
	.param .u64 _Z32vecAddUnrolledBy4ILPMaximizationPfS_S_i_param_1,
	.param .u64 _Z32vecAddUnrolledBy4ILPMaximizationPfS_S_i_param_2,
	.param .u32 _Z32vecAddUnrolledBy4ILPMaximizationPfS_S_i_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd4, [_Z32vecAddUnrolledBy4ILPMaximizationPfS_S_i_param_0];
	ld.param.u64 	%rd5, [_Z32vecAddUnrolledBy4ILPMaximizationPfS_S_i_param_1];
	ld.param.u64 	%rd6, [_Z32vecAddUnrolledBy4ILPMaximizationPfS_S_i_param_2];
	ld.param.u32 	%r6, [_Z32vecAddUnrolledBy4ILPMaximizationPfS_S_i_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r14, %r7, %r1, %r8;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB2_3;

	cvta.to.global.u64 	%rd1, %rd6;
	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd3, %rd4;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r10, %r1, %r9;
	shl.b32 	%r3, %r10, 2;

$L__BB2_2:
	mul.wide.u32 	%rd7, %r14, 4;
	add.s64 	%rd8, %rd3, %rd7;
	add.s32 	%r11, %r14, 1;
	mul.wide.u32 	%rd9, %r11, 4;
	add.s64 	%rd10, %rd3, %rd9;
	ld.global.f32 	%f1, [%rd10];
	add.s32 	%r12, %r14, 2;
	mul.wide.u32 	%rd11, %r12, 4;
	add.s64 	%rd12, %rd3, %rd11;
	ld.global.f32 	%f2, [%rd12];
	add.s32 	%r13, %r14, 3;
	mul.wide.u32 	%rd13, %r13, 4;
	add.s64 	%rd14, %rd3, %rd13;
	ld.global.f32 	%f3, [%rd14];
	add.s64 	%rd15, %rd2, %rd7;
	add.s64 	%rd16, %rd2, %rd9;
	ld.global.f32 	%f4, [%rd16];
	add.s64 	%rd17, %rd2, %rd11;
	ld.global.f32 	%f5, [%rd17];
	add.s64 	%rd18, %rd2, %rd13;
	ld.global.f32 	%f6, [%rd18];
	ld.global.f32 	%f7, [%rd15];
	ld.global.f32 	%f8, [%rd8];
	add.f32 	%f9, %f8, %f7;
	add.s64 	%rd19, %rd1, %rd7;
	st.global.f32 	[%rd19], %f9;
	add.f32 	%f10, %f1, %f4;
	add.s64 	%rd20, %rd1, %rd9;
	st.global.f32 	[%rd20], %f10;
	add.f32 	%f11, %f2, %f5;
	add.s64 	%rd21, %rd1, %rd11;
	st.global.f32 	[%rd21], %f11;
	add.f32 	%f12, %f3, %f6;
	add.s64 	%rd22, %rd1, %rd13;
	st.global.f32 	[%rd22], %f12;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p2, %r14, %r6;
	@%p2 bra 	$L__BB2_2;

$L__BB2_3:
	ret;

}
	// .globl	_Z17euclideanDistancePfS_S_i
.visible .entry _Z17euclideanDistancePfS_S_i(
	.param .u64 _Z17euclideanDistancePfS_S_i_param_0,
	.param .u64 _Z17euclideanDistancePfS_S_i_param_1,
	.param .u64 _Z17euclideanDistancePfS_S_i_param_2,
	.param .u32 _Z17euclideanDistancePfS_S_i_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd4, [_Z17euclideanDistancePfS_S_i_param_0];
	ld.param.u64 	%rd5, [_Z17euclideanDistancePfS_S_i_param_1];
	ld.param.u64 	%rd6, [_Z17euclideanDistancePfS_S_i_param_2];
	ld.param.u32 	%r6, [_Z17euclideanDistancePfS_S_i_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r7, %r1, %r8;
	setp.ge.u32 	%p1, %r10, %r6;
	@%p1 bra 	$L__BB3_3;

	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd3, %rd6;

$L__BB3_2:
	mul.wide.u32 	%rd7, %r10, 4;
	add.s64 	%rd8, %rd1, %rd7;
	add.s64 	%rd9, %rd2, %rd7;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd9];
	mul.f32 	%f3, %f2, %f2;
	fma.rn.f32 	%f4, %f1, %f1, %f3;
	sqrt.rn.f32 	%f5, %f4;
	add.s64 	%rd10, %rd3, %rd7;
	st.global.f32 	[%rd10], %f5;
	add.s32 	%r10, %r10, %r3;
	setp.lt.u32 	%p2, %r10, %r6;
	@%p2 bra 	$L__BB3_2;

$L__BB3_3:
	ret;

}
	// .globl	_Z23euclideanDistanceApproxPfS_S_i
.visible .entry _Z23euclideanDistanceApproxPfS_S_i(
	.param .u64 _Z23euclideanDistanceApproxPfS_S_i_param_0,
	.param .u64 _Z23euclideanDistanceApproxPfS_S_i_param_1,
	.param .u64 _Z23euclideanDistanceApproxPfS_S_i_param_2,
	.param .u32 _Z23euclideanDistanceApproxPfS_S_i_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd4, [_Z23euclideanDistanceApproxPfS_S_i_param_0];
	ld.param.u64 	%rd5, [_Z23euclideanDistanceApproxPfS_S_i_param_1];
	ld.param.u64 	%rd6, [_Z23euclideanDistanceApproxPfS_S_i_param_2];
	ld.param.u32 	%r6, [_Z23euclideanDistanceApproxPfS_S_i_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r7, %r1, %r8;
	setp.ge.u32 	%p1, %r10, %r6;
	@%p1 bra 	$L__BB4_3;

	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd3, %rd6;

$L__BB4_2:
	mul.wide.u32 	%rd7, %r10, 4;
	add.s64 	%rd8, %rd1, %rd7;
	add.s64 	%rd9, %rd2, %rd7;
	add.s64 	%rd10, %rd3, %rd7;
	ld.global.f32 	%f3, [%rd8];
	ld.global.f32 	%f4, [%rd9];
	mul.f32 	%f5, %f4, %f4;
	fma.rn.f32 	%f2, %f3, %f3, %f5;
	// begin inline asm
	sqrt.approx.f32 %f1, %f2;
	// end inline asm
	st.global.f32 	[%rd10], %f1;
	add.s32 	%r10, %r10, %r3;
	setp.lt.u32 	%p2, %r10, %r6;
	@%p2 bra 	$L__BB4_2;

$L__BB4_3:
	ret;

}
	// .globl	_Z26vecAddUnrolledBy4PipelinedILi4EEvPfS0_S0_i
.visible .entry _Z26vecAddUnrolledBy4PipelinedILi4EEvPfS0_S0_i(
	.param .u64 _Z26vecAddUnrolledBy4PipelinedILi4EEvPfS0_S0_i_param_0,
	.param .u64 _Z26vecAddUnrolledBy4PipelinedILi4EEvPfS0_S0_i_param_1,
	.param .u64 _Z26vecAddUnrolledBy4PipelinedILi4EEvPfS0_S0_i_param_2,
	.param .u32 _Z26vecAddUnrolledBy4PipelinedILi4EEvPfS0_S0_i_param_3
)
{
	.reg .pred 	%p<26>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<156>;
	.reg .b64 	%rd<76>;


	ld.param.u64 	%rd6, [_Z26vecAddUnrolledBy4PipelinedILi4EEvPfS0_S0_i_param_0];
	ld.param.u64 	%rd7, [_Z26vecAddUnrolledBy4PipelinedILi4EEvPfS0_S0_i_param_1];
	ld.param.u64 	%rd5, [_Z26vecAddUnrolledBy4PipelinedILi4EEvPfS0_S0_i_param_2];
	ld.param.u32 	%r23, [_Z26vecAddUnrolledBy4PipelinedILi4EEvPfS0_S0_i_param_3];
	mov.u32 	%r24, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %tid.x;
	mad.lo.s32 	%r155, %r24, %r1, %r2;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r4, %r25, %r1;
	shl.b32 	%r5, %r1, 4;
	cvta.to.global.u64 	%rd1, %rd6;
	cvta.to.global.u64 	%rd2, %rd7;
	setp.ge.u32 	%p1, %r155, %r23;
	@%p1 bra 	$L__BB5_2;

	shl.b32 	%r28, %r2, 2;
	mov.u32 	%r29, shared;
	add.s32 	%r26, %r29, %r28;
	mul.wide.u32 	%rd10, %r155, 4;
	add.s64 	%rd8, %rd1, %rd10;
	// begin inline asm
	cp.async.ca.shared.global [%r26], [%rd8], 4, 4;
	// end inline asm
	shl.b32 	%r30, %r5, 2;
	add.s32 	%r27, %r26, %r30;
	add.s64 	%rd9, %rd2, %rd10;
	// begin inline asm
	cp.async.ca.shared.global [%r27], [%rd9], 4, 4;
	// end inline asm

$L__BB5_2:
	add.s32 	%r6, %r4, %r155;
	setp.ge.u32 	%p2, %r6, %r23;
	@%p2 bra 	$L__BB5_4;

	add.s32 	%r33, %r1, %r2;
	shl.b32 	%r34, %r33, 2;
	mov.u32 	%r35, shared;
	add.s32 	%r31, %r35, %r34;
	mul.wide.u32 	%rd13, %r6, 4;
	add.s64 	%rd11, %rd1, %rd13;
	// begin inline asm
	cp.async.ca.shared.global [%r31], [%rd11], 4, 4;
	// end inline asm
	shl.b32 	%r36, %r5, 2;
	add.s32 	%r32, %r31, %r36;
	add.s64 	%rd12, %rd2, %rd13;
	// begin inline asm
	cp.async.ca.shared.global [%r32], [%rd12], 4, 4;
	// end inline asm

$L__BB5_4:
	add.s32 	%r7, %r6, %r4;
	setp.ge.u32 	%p3, %r7, %r23;
	@%p3 bra 	$L__BB5_6;

	shl.b32 	%r39, %r1, 1;
	add.s32 	%r40, %r39, %r2;
	shl.b32 	%r41, %r40, 2;
	mov.u32 	%r42, shared;
	add.s32 	%r37, %r42, %r41;
	mul.wide.u32 	%rd16, %r7, 4;
	add.s64 	%rd14, %rd1, %rd16;
	// begin inline asm
	cp.async.ca.shared.global [%r37], [%rd14], 4, 4;
	// end inline asm
	shl.b32 	%r43, %r5, 2;
	add.s32 	%r38, %r37, %r43;
	add.s64 	%rd15, %rd2, %rd16;
	// begin inline asm
	cp.async.ca.shared.global [%r38], [%rd15], 4, 4;
	// end inline asm

$L__BB5_6:
	add.s32 	%r8, %r7, %r4;
	setp.ge.u32 	%p4, %r8, %r23;
	@%p4 bra 	$L__BB5_8;

	mad.lo.s32 	%r46, %r1, 3, %r2;
	shl.b32 	%r47, %r46, 2;
	mov.u32 	%r48, shared;
	add.s32 	%r44, %r48, %r47;
	mul.wide.u32 	%rd19, %r8, 4;
	add.s64 	%rd17, %rd1, %rd19;
	// begin inline asm
	cp.async.ca.shared.global [%r44], [%rd17], 4, 4;
	// end inline asm
	shl.b32 	%r49, %r5, 2;
	add.s32 	%r45, %r44, %r49;
	add.s64 	%rd18, %rd2, %rd19;
	// begin inline asm
	cp.async.ca.shared.global [%r45], [%rd18], 4, 4;
	// end inline asm

$L__BB5_8:
	// begin inline asm
	cp.async.commit_group;
	// end inline asm
	@%p1 bra 	$L__BB5_10;

	shl.b32 	%r52, %r1, 2;
	add.s32 	%r53, %r52, %r2;
	shl.b32 	%r54, %r53, 2;
	mov.u32 	%r55, shared;
	add.s32 	%r50, %r55, %r54;
	mul.wide.u32 	%rd22, %r155, 4;
	add.s64 	%rd20, %rd1, %rd22;
	// begin inline asm
	cp.async.ca.shared.global [%r50], [%rd20], 4, 4;
	// end inline asm
	shl.b32 	%r56, %r5, 2;
	add.s32 	%r51, %r50, %r56;
	add.s64 	%rd21, %rd2, %rd22;
	// begin inline asm
	cp.async.ca.shared.global [%r51], [%rd21], 4, 4;
	// end inline asm

$L__BB5_10:
	@%p2 bra 	$L__BB5_12;

	mad.lo.s32 	%r59, %r1, 5, %r2;
	shl.b32 	%r60, %r59, 2;
	mov.u32 	%r61, shared;
	add.s32 	%r57, %r61, %r60;
	mul.wide.u32 	%rd25, %r6, 4;
	add.s64 	%rd23, %rd1, %rd25;
	// begin inline asm
	cp.async.ca.shared.global [%r57], [%rd23], 4, 4;
	// end inline asm
	shl.b32 	%r62, %r5, 2;
	add.s32 	%r58, %r57, %r62;
	add.s64 	%rd24, %rd2, %rd25;
	// begin inline asm
	cp.async.ca.shared.global [%r58], [%rd24], 4, 4;
	// end inline asm

$L__BB5_12:
	@%p3 bra 	$L__BB5_14;

	mad.lo.s32 	%r65, %r1, 6, %r2;
	shl.b32 	%r66, %r65, 2;
	mov.u32 	%r67, shared;
	add.s32 	%r63, %r67, %r66;
	mul.wide.u32 	%rd28, %r7, 4;
	add.s64 	%rd26, %rd1, %rd28;
	// begin inline asm
	cp.async.ca.shared.global [%r63], [%rd26], 4, 4;
	// end inline asm
	shl.b32 	%r68, %r5, 2;
	add.s32 	%r64, %r63, %r68;
	add.s64 	%rd27, %rd2, %rd28;
	// begin inline asm
	cp.async.ca.shared.global [%r64], [%rd27], 4, 4;
	// end inline asm

$L__BB5_14:
	@%p4 bra 	$L__BB5_16;

	mad.lo.s32 	%r71, %r1, 7, %r2;
	shl.b32 	%r72, %r71, 2;
	mov.u32 	%r73, shared;
	add.s32 	%r69, %r73, %r72;
	mul.wide.u32 	%rd31, %r8, 4;
	add.s64 	%rd29, %rd1, %rd31;
	// begin inline asm
	cp.async.ca.shared.global [%r69], [%rd29], 4, 4;
	// end inline asm
	shl.b32 	%r74, %r5, 2;
	add.s32 	%r70, %r69, %r74;
	add.s64 	%rd30, %rd2, %rd31;
	// begin inline asm
	cp.async.ca.shared.global [%r70], [%rd30], 4, 4;
	// end inline asm

$L__BB5_16:
	// begin inline asm
	cp.async.commit_group;
	// end inline asm
	@%p1 bra 	$L__BB5_18;

	shl.b32 	%r77, %r1, 3;
	add.s32 	%r78, %r77, %r2;
	shl.b32 	%r79, %r78, 2;
	mov.u32 	%r80, shared;
	add.s32 	%r75, %r80, %r79;
	mul.wide.u32 	%rd34, %r155, 4;
	add.s64 	%rd32, %rd1, %rd34;
	// begin inline asm
	cp.async.ca.shared.global [%r75], [%rd32], 4, 4;
	// end inline asm
	shl.b32 	%r81, %r5, 2;
	add.s32 	%r76, %r75, %r81;
	add.s64 	%rd33, %rd2, %rd34;
	// begin inline asm
	cp.async.ca.shared.global [%r76], [%rd33], 4, 4;
	// end inline asm

$L__BB5_18:
	@%p2 bra 	$L__BB5_20;

	mad.lo.s32 	%r84, %r1, 9, %r2;
	shl.b32 	%r85, %r84, 2;
	mov.u32 	%r86, shared;
	add.s32 	%r82, %r86, %r85;
	mul.wide.u32 	%rd37, %r6, 4;
	add.s64 	%rd35, %rd1, %rd37;
	// begin inline asm
	cp.async.ca.shared.global [%r82], [%rd35], 4, 4;
	// end inline asm
	shl.b32 	%r87, %r5, 2;
	add.s32 	%r83, %r82, %r87;
	add.s64 	%rd36, %rd2, %rd37;
	// begin inline asm
	cp.async.ca.shared.global [%r83], [%rd36], 4, 4;
	// end inline asm

$L__BB5_20:
	@%p3 bra 	$L__BB5_22;

	mad.lo.s32 	%r90, %r1, 10, %r2;
	shl.b32 	%r91, %r90, 2;
	mov.u32 	%r92, shared;
	add.s32 	%r88, %r92, %r91;
	mul.wide.u32 	%rd40, %r7, 4;
	add.s64 	%rd38, %rd1, %rd40;
	// begin inline asm
	cp.async.ca.shared.global [%r88], [%rd38], 4, 4;
	// end inline asm
	shl.b32 	%r93, %r5, 2;
	add.s32 	%r89, %r88, %r93;
	add.s64 	%rd39, %rd2, %rd40;
	// begin inline asm
	cp.async.ca.shared.global [%r89], [%rd39], 4, 4;
	// end inline asm

$L__BB5_22:
	@%p4 bra 	$L__BB5_24;

	mad.lo.s32 	%r96, %r1, 11, %r2;
	shl.b32 	%r97, %r96, 2;
	mov.u32 	%r98, shared;
	add.s32 	%r94, %r98, %r97;
	mul.wide.u32 	%rd43, %r8, 4;
	add.s64 	%rd41, %rd1, %rd43;
	// begin inline asm
	cp.async.ca.shared.global [%r94], [%rd41], 4, 4;
	// end inline asm
	shl.b32 	%r99, %r5, 2;
	add.s32 	%r95, %r94, %r99;
	add.s64 	%rd42, %rd2, %rd43;
	// begin inline asm
	cp.async.ca.shared.global [%r95], [%rd42], 4, 4;
	// end inline asm

$L__BB5_24:
	// begin inline asm
	cp.async.commit_group;
	// end inline asm
	@%p1 bra 	$L__BB5_26;

	mad.lo.s32 	%r102, %r1, 12, %r2;
	shl.b32 	%r103, %r102, 2;
	mov.u32 	%r104, shared;
	add.s32 	%r100, %r104, %r103;
	mul.wide.u32 	%rd46, %r155, 4;
	add.s64 	%rd44, %rd1, %rd46;
	// begin inline asm
	cp.async.ca.shared.global [%r100], [%rd44], 4, 4;
	// end inline asm
	shl.b32 	%r105, %r5, 2;
	add.s32 	%r101, %r100, %r105;
	add.s64 	%rd45, %rd2, %rd46;
	// begin inline asm
	cp.async.ca.shared.global [%r101], [%rd45], 4, 4;
	// end inline asm

$L__BB5_26:
	@%p2 bra 	$L__BB5_28;

	mad.lo.s32 	%r108, %r1, 13, %r2;
	shl.b32 	%r109, %r108, 2;
	mov.u32 	%r110, shared;
	add.s32 	%r106, %r110, %r109;
	mul.wide.u32 	%rd49, %r6, 4;
	add.s64 	%rd47, %rd1, %rd49;
	// begin inline asm
	cp.async.ca.shared.global [%r106], [%rd47], 4, 4;
	// end inline asm
	shl.b32 	%r111, %r5, 2;
	add.s32 	%r107, %r106, %r111;
	add.s64 	%rd48, %rd2, %rd49;
	// begin inline asm
	cp.async.ca.shared.global [%r107], [%rd48], 4, 4;
	// end inline asm

$L__BB5_28:
	@%p3 bra 	$L__BB5_30;

	mad.lo.s32 	%r114, %r1, 14, %r2;
	shl.b32 	%r115, %r114, 2;
	mov.u32 	%r116, shared;
	add.s32 	%r112, %r116, %r115;
	mul.wide.u32 	%rd52, %r7, 4;
	add.s64 	%rd50, %rd1, %rd52;
	// begin inline asm
	cp.async.ca.shared.global [%r112], [%rd50], 4, 4;
	// end inline asm
	shl.b32 	%r117, %r5, 2;
	add.s32 	%r113, %r112, %r117;
	add.s64 	%rd51, %rd2, %rd52;
	// begin inline asm
	cp.async.ca.shared.global [%r113], [%rd51], 4, 4;
	// end inline asm

$L__BB5_30:
	@%p4 bra 	$L__BB5_32;

	mad.lo.s32 	%r120, %r1, 15, %r2;
	shl.b32 	%r121, %r120, 2;
	mov.u32 	%r122, shared;
	add.s32 	%r118, %r122, %r121;
	mul.wide.u32 	%rd55, %r8, 4;
	add.s64 	%rd53, %rd1, %rd55;
	// begin inline asm
	cp.async.ca.shared.global [%r118], [%rd53], 4, 4;
	// end inline asm
	shl.b32 	%r123, %r5, 2;
	add.s32 	%r119, %r118, %r123;
	add.s64 	%rd54, %rd2, %rd55;
	// begin inline asm
	cp.async.ca.shared.global [%r119], [%rd54], 4, 4;
	// end inline asm

$L__BB5_32:
	// begin inline asm
	cp.async.commit_group;
	// end inline asm
	cvta.to.global.u64 	%rd3, %rd5;
	@%p1 bra 	$L__BB5_49;

	shl.b32 	%r9, %r1, 2;
	mov.u32 	%r154, 0;

$L__BB5_34:
	// begin inline asm
	cp.async.wait_group 3;
	// end inline asm
	shl.b32 	%r125, %r154, 2;
	mad.lo.s32 	%r126, %r125, %r1, %r2;
	shl.b32 	%r127, %r126, 2;
	mov.u32 	%r128, shared;
	add.s32 	%r12, %r128, %r127;
	shl.b32 	%r129, %r5, 2;
	add.s32 	%r13, %r12, %r129;
	add.s32 	%r14, %r12, %r9;
	add.s32 	%r15, %r13, %r9;
	add.s32 	%r130, %r14, %r9;
	ld.shared.f32 	%f1, [%r130];
	add.s32 	%r131, %r15, %r9;
	ld.shared.f32 	%f2, [%r131];
	add.s32 	%r132, %r130, %r9;
	ld.shared.f32 	%f3, [%r132];
	add.s32 	%r133, %r131, %r9;
	ld.shared.f32 	%f4, [%r133];
	ld.shared.f32 	%f5, [%r13];
	ld.shared.f32 	%f6, [%r12];
	add.f32 	%f7, %f6, %f5;
	cvt.u64.u32 	%rd4, %r155;
	mul.wide.u32 	%rd56, %r155, 4;
	add.s64 	%rd57, %rd3, %rd56;
	st.global.f32 	[%rd57], %f7;
	add.s32 	%r16, %r155, %r4;
	setp.ge.u32 	%p18, %r16, %r23;
	@%p18 bra 	$L__BB5_36;

	ld.shared.f32 	%f8, [%r14];
	ld.shared.f32 	%f9, [%r15];
	add.f32 	%f10, %f8, %f9;
	mul.wide.u32 	%rd58, %r16, 4;
	add.s64 	%rd59, %rd3, %rd58;
	st.global.f32 	[%rd59], %f10;

$L__BB5_36:
	add.s32 	%r17, %r16, %r4;
	setp.ge.u32 	%p19, %r17, %r23;
	@%p19 bra 	$L__BB5_38;

	add.f32 	%f11, %f1, %f2;
	mul.wide.u32 	%rd60, %r17, 4;
	add.s64 	%rd61, %rd3, %rd60;
	st.global.f32 	[%rd61], %f11;

$L__BB5_38:
	add.s32 	%r18, %r17, %r4;
	setp.ge.u32 	%p20, %r18, %r23;
	@%p20 bra 	$L__BB5_40;

	add.f32 	%f12, %f3, %f4;
	mul.wide.u32 	%rd62, %r18, 4;
	add.s64 	%rd63, %rd3, %rd62;
	st.global.f32 	[%rd63], %f12;

$L__BB5_40:
	cvt.u32.u64 	%r134, %rd4;
	setp.ge.u32 	%p21, %r134, %r23;
	@%p21 bra 	$L__BB5_42;

	shl.b64 	%rd66, %rd4, 2;
	add.s64 	%rd64, %rd1, %rd66;
	// begin inline asm
	cp.async.ca.shared.global [%r12], [%rd64], 4, 4;
	// end inline asm
	add.s64 	%rd65, %rd2, %rd66;
	// begin inline asm
	cp.async.ca.shared.global [%r13], [%rd65], 4, 4;
	// end inline asm

$L__BB5_42:
	@%p18 bra 	$L__BB5_44;

	mul.wide.u32 	%rd69, %r16, 4;
	add.s64 	%rd67, %rd1, %rd69;
	// begin inline asm
	cp.async.ca.shared.global [%r14], [%rd67], 4, 4;
	// end inline asm
	add.s64 	%rd68, %rd2, %rd69;
	add.s32 	%r139, %r14, %r129;
	// begin inline asm
	cp.async.ca.shared.global [%r139], [%rd68], 4, 4;
	// end inline asm

$L__BB5_44:
	@%p19 bra 	$L__BB5_46;

	mul.wide.u32 	%rd72, %r17, 4;
	add.s64 	%rd70, %rd1, %rd72;
	// begin inline asm
	cp.async.ca.shared.global [%r130], [%rd70], 4, 4;
	// end inline asm
	add.s64 	%rd71, %rd2, %rd72;
	add.s32 	%r143, %r130, %r129;
	// begin inline asm
	cp.async.ca.shared.global [%r143], [%rd71], 4, 4;
	// end inline asm

$L__BB5_46:
	@%p20 bra 	$L__BB5_48;

	mul.wide.u32 	%rd75, %r18, 4;
	add.s64 	%rd73, %rd1, %rd75;
	// begin inline asm
	cp.async.ca.shared.global [%r132], [%rd73], 4, 4;
	// end inline asm
	add.s64 	%rd74, %rd2, %rd75;
	add.s32 	%r146, %r132, %r129;
	// begin inline asm
	cp.async.ca.shared.global [%r146], [%rd74], 4, 4;
	// end inline asm

$L__BB5_48:
	// begin inline asm
	cp.async.commit_group;
	// end inline asm
	add.s32 	%r149, %r154, 1;
	shr.s32 	%r150, %r149, 31;
	shr.u32 	%r151, %r150, 30;
	add.s32 	%r152, %r149, %r151;
	and.b32  	%r153, %r152, -4;
	sub.s32 	%r154, %r149, %r153;
	add.s32 	%r155, %r18, %r4;
	setp.lt.u32 	%p25, %r155, %r23;
	@%p25 bra 	$L__BB5_34;

$L__BB5_49:
	ret;

}

